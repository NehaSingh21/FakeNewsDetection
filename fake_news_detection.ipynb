{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "import seaborn as sb\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models.doc2vec import TaggedDocument \n",
    "from gensim.models import Doc2Vec\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "plt.style.use('dark_background')\n",
    "\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import time\n",
    "\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Reading the data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data read from 'raw_data.pkl'.\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(\"raw_data.pkl\"): \n",
    "    with open(\"raw_data.pkl\", \"br\") as f:\n",
    "        raw_data = pickle.load(f)\n",
    "    print(\"Raw data read from 'raw_data.pkl'.\")\n",
    "else:\n",
    "    fake_news = pd.read_csv(\"fake.csv.zip\", parse_dates=['published'])\n",
    "    articles1 = pd.read_csv(\"articles1.csv.zip\")\n",
    "    articles2 = pd.read_csv(\"articles2.csv.zip\")\n",
    "    articles3 = pd.read_csv(\"articles3.csv.zip\")\n",
    "    \n",
    "    true_news = pd.concat([articles1, articles2, articles3])\n",
    "    true_news = true_news[['publication', 'title', 'author', 'content']]\n",
    "    true_news = true_news.loc[true_news.publication.isin(['New York Times'])]\n",
    "    true_news = true_news[['title', 'author', 'content']]\n",
    "    true_news['label'] = 0\n",
    "    \n",
    "    tz = pytz.timezone('America/Chicago')\n",
    "    fake_news = fake_news.loc[fake_news.language == 'english']\n",
    "    fake_news = fake_news.loc[fake_news.published > tz.localize(datetime.strptime('2016-11-01', '%Y-%m-%d'))]\n",
    "    fake_news = fake_news[['title', 'author', 'text']]\n",
    "    fake_news.rename(columns={'text':'content'}, inplace=True)\n",
    "    fake_news['label'] = 1\n",
    "    \n",
    "    raw_data = pd.concat([true_news, fake_news])\n",
    "    raw_data = raw_data.loc[(raw_data.title.isna() == False) & (raw_data.content.isna() == False)]\n",
    "    raw_data = raw_data.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    with open(\"raw_data.pkl\", \"bw\") as f:\n",
    "        pickle.dump(raw_data, f)\n",
    "    print(\"Raw data written to 'raw_data.pkl'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>John Kerry’s Trip to the South Pole: Nazi Root...</td>\n",
       "      <td>Author</td>\n",
       "      <td>Region: USA in the World So why did the US Sec...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>John Kerry Is Said to Side With Diplomats’ Cri...</td>\n",
       "      <td>Mark Landler</td>\n",
       "      <td>WASHINGTON  —   For a cabinet member whose dep...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Tim Kaine Compares Donald Trump’s Comments on ...</td>\n",
       "      <td>Yamiche Alcindor</td>\n",
       "      <td>Senator Tim Kaine of Virginia on Sunday compar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Revel in the Bounty of Spring, With a Feast Fr...</td>\n",
       "      <td>Sam Sifton</td>\n",
       "      <td>The first thing Yotam Ottolenghi did before he...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>In Betsy DeVos for Education, Trump Taps Into ...</td>\n",
       "      <td>Vanessa Friedman, Maggie Haberman and Alan Rap...</td>\n",
       "      <td>Donald J. Trump has reached into Western Mic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  John Kerry’s Trip to the South Pole: Nazi Root...   \n",
       "1  John Kerry Is Said to Side With Diplomats’ Cri...   \n",
       "2  Tim Kaine Compares Donald Trump’s Comments on ...   \n",
       "3  Revel in the Bounty of Spring, With a Feast Fr...   \n",
       "4  In Betsy DeVos for Education, Trump Taps Into ...   \n",
       "\n",
       "                                              author  \\\n",
       "0                                             Author   \n",
       "1                                       Mark Landler   \n",
       "2                                   Yamiche Alcindor   \n",
       "3                                         Sam Sifton   \n",
       "4  Vanessa Friedman, Maggie Haberman and Alan Rap...   \n",
       "\n",
       "                                             content  label  \n",
       "0  Region: USA in the World So why did the US Sec...      1  \n",
       "1  WASHINGTON  —   For a cabinet member whose dep...      0  \n",
       "2  Senator Tim Kaine of Virginia on Sunday compar...      0  \n",
       "3  The first thing Yotam Ottolenghi did before he...      0  \n",
       "4    Donald J. Trump has reached into Western Mic...      0  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Preprocessing__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(corpus):\n",
    "    corpus = [[word for word in simple_preprocess(doc, min_len=3, deacc=True) if word not in stop_words] for doc in corpus]\n",
    "    return [' '.join(word) for word in corpus]\n",
    "\n",
    "def lemmatize(corpus):\n",
    "    corpus = [[token.lemma_ for token in doc] for doc in nlp.pipe(corpus, batch_size=3000, n_threads=-1)]\n",
    "    return [' '.join(word) for word in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data written to 'preprocessed_data.pkl'.\n"
     ]
    }
   ],
   "source": [
    "time_ = datetime.now()\n",
    "if os.path.isfile(\"preprocessed_data.pkl\"): \n",
    "    with open(\"preprocessed_data.pkl\", \"br\") as f:\n",
    "        preprocessed_data = pickle.load(f)\n",
    "    print(\"Preprocessed data read from 'preprocessed_data.pkl'.\")\n",
    "    \n",
    "else:\n",
    "    preprocessed_data = pd.DataFrame(columns=['title', 'author', 'content'], index=raw_data.index)\n",
    "    \n",
    "    preprocessed_data['title'] = remove_stopwords(raw_data.title)\n",
    "    preprocessed_data['content'] = remove_stopwords(raw_data.content)\n",
    "    preprocessed_data['author'] = raw_data.author\n",
    "    preprocessed_data['label'] = raw_data.label\n",
    "    \n",
    "    preprocessed_data['title'] = lemmatize(preprocessed_data.title)\n",
    "    preprocessed_data['content'] = lemmatize(preprocessed_data.content)\n",
    "    \n",
    "    with open(\"preprocessed_data.pkl\", \"bw\") as f:\n",
    "        pickle.dump(preprocessed_data, f)\n",
    "    print(\"Preprocessed data written to 'preprocessed_data.pkl'.\")\n",
    "\n",
    "time_ = datetime.now() - time_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.timedelta(seconds=1815, microseconds=54989)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constructTaggedDocument(data):\n",
    "    sentences = []\n",
    "    for i, text in data.iteritems():\n",
    "        sentences.append(TaggedDocument(text.split(), ['Train_' + str(i)]))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = constructTaggedDocument(preprocessed_data.title)\n",
    "y = preprocessed_data.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-16 16:25:44,594 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "2020-01-16 16:25:44,605 : INFO : collecting all words and their counts\n",
      "2020-01-16 16:25:44,608 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2020-01-16 16:25:44,675 : INFO : PROGRESS: at example #10000, processed 87222 words (1339744/s), 11432 word types, 10000 tags\n",
      "2020-01-16 16:25:44,696 : INFO : collected 13306 word types and 13582 unique tags from a corpus of 13582 examples and 118284 words\n",
      "2020-01-16 16:25:44,697 : INFO : Loading a fresh vocabulary\n",
      "2020-01-16 16:25:44,717 : INFO : effective_min_count=2 retains 7051 unique words (52% of original 13306, drops 6255)\n",
      "2020-01-16 16:25:44,718 : INFO : effective_min_count=2 leaves 112029 word corpus (94% of original 118284, drops 6255)\n",
      "2020-01-16 16:25:44,758 : INFO : deleting the raw counts dictionary of 13306 items\n",
      "2020-01-16 16:25:44,760 : INFO : sample=0.0001 downsamples 629 most-common words\n",
      "2020-01-16 16:25:44,761 : INFO : downsampling leaves estimated 64700 word corpus (57.8% of prior 112029)\n",
      "2020-01-16 16:25:44,792 : INFO : estimated required memory for 7051 words and 50 dimensions: 11778700 bytes\n",
      "2020-01-16 16:25:44,793 : INFO : resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "d2v_model = Doc2Vec(min_count=2, window=5, vector_size=50, sample=1e-4, workers=5, epochs=10)\n",
    "d2v_model.build_vocab(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-16 16:27:39,589 : INFO : training model with 5 workers on 7051 vocabulary and 50 features, using sg=0 hs=0 sample=0.0001 negative=5 window=5\n",
      "2020-01-16 16:27:41,054 : INFO : EPOCH 1 - PROGRESS: at 50.96% examples, 27628 words/s, in_qsize 6, out_qsize 0\n",
      "2020-01-16 16:27:41,112 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-01-16 16:27:41,115 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-16 16:27:41,130 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-16 16:27:41,306 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-16 16:27:41,315 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-16 16:27:41,315 : INFO : EPOCH - 1 : training on 118284 raw words (78255 effective words) took 1.7s, 46113 effective words/s\n",
      "2020-01-16 16:27:42,345 : INFO : EPOCH 2 - PROGRESS: at 42.44% examples, 32590 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-16 16:27:43,056 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-01-16 16:27:43,079 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-16 16:27:43,095 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-16 16:27:43,256 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-16 16:27:43,268 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-16 16:27:43,269 : INFO : EPOCH - 2 : training on 118284 raw words (78227 effective words) took 1.9s, 40369 effective words/s\n",
      "2020-01-16 16:27:44,843 : INFO : EPOCH 3 - PROGRESS: at 50.74% examples, 25615 words/s, in_qsize 6, out_qsize 0\n",
      "2020-01-16 16:27:44,885 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-01-16 16:27:44,913 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-16 16:27:44,933 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-16 16:27:45,078 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-16 16:27:45,097 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-16 16:27:45,098 : INFO : EPOCH - 3 : training on 118284 raw words (78299 effective words) took 1.8s, 43485 effective words/s\n",
      "2020-01-16 16:27:46,611 : INFO : EPOCH 4 - PROGRESS: at 50.74% examples, 26375 words/s, in_qsize 6, out_qsize 0\n",
      "2020-01-16 16:27:46,660 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-01-16 16:27:46,706 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-16 16:27:46,710 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-16 16:27:46,875 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-16 16:27:46,910 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-16 16:27:46,910 : INFO : EPOCH - 4 : training on 118284 raw words (78090 effective words) took 1.8s, 43467 effective words/s\n",
      "2020-01-16 16:27:48,350 : INFO : EPOCH 5 - PROGRESS: at 50.74% examples, 27745 words/s, in_qsize 6, out_qsize 0\n",
      "2020-01-16 16:27:48,370 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-01-16 16:27:48,390 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-16 16:27:48,409 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-16 16:27:48,585 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-16 16:27:48,590 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-16 16:27:48,590 : INFO : EPOCH - 5 : training on 118284 raw words (78026 effective words) took 1.7s, 46926 effective words/s\n",
      "2020-01-16 16:27:50,012 : INFO : EPOCH 6 - PROGRESS: at 50.74% examples, 28399 words/s, in_qsize 6, out_qsize 0\n",
      "2020-01-16 16:27:50,041 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-01-16 16:27:50,052 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-16 16:27:50,073 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-16 16:27:50,244 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-16 16:27:50,257 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-16 16:27:50,259 : INFO : EPOCH - 6 : training on 118284 raw words (78243 effective words) took 1.6s, 47846 effective words/s\n",
      "2020-01-16 16:27:51,559 : INFO : EPOCH 7 - PROGRESS: at 50.85% examples, 31051 words/s, in_qsize 6, out_qsize 0\n",
      "2020-01-16 16:27:51,563 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-01-16 16:27:51,575 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-16 16:27:51,580 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-16 16:27:51,751 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-16 16:27:51,765 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-16 16:27:51,766 : INFO : EPOCH - 7 : training on 118284 raw words (78016 effective words) took 1.5s, 52595 effective words/s\n",
      "2020-01-16 16:27:52,800 : INFO : EPOCH 8 - PROGRESS: at 59.20% examples, 45431 words/s, in_qsize 5, out_qsize 0\n",
      "2020-01-16 16:27:52,956 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-01-16 16:27:52,963 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-16 16:27:53,003 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-16 16:27:53,104 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-16 16:27:53,109 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-16 16:27:53,110 : INFO : EPOCH - 8 : training on 118284 raw words (78181 effective words) took 1.3s, 58995 effective words/s\n",
      "2020-01-16 16:27:54,390 : INFO : EPOCH 9 - PROGRESS: at 50.90% examples, 31292 words/s, in_qsize 6, out_qsize 0\n",
      "2020-01-16 16:27:54,442 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-01-16 16:27:54,449 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-16 16:27:54,467 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-16 16:27:54,639 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-16 16:27:54,646 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-16 16:27:54,646 : INFO : EPOCH - 9 : training on 118284 raw words (78437 effective words) took 1.5s, 51482 effective words/s\n",
      "2020-01-16 16:27:55,923 : INFO : EPOCH 10 - PROGRESS: at 50.90% examples, 31475 words/s, in_qsize 6, out_qsize 0\n",
      "2020-01-16 16:27:55,972 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-01-16 16:27:55,977 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-16 16:27:55,992 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-16 16:27:56,155 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-16 16:27:56,168 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-16 16:27:56,169 : INFO : EPOCH - 10 : training on 118284 raw words (78198 effective words) took 1.5s, 52004 effective words/s\n",
      "2020-01-16 16:27:56,171 : INFO : training on a 1182840 raw words (781972 effective words) took 16.6s, 47172 effective words/s\n"
     ]
    }
   ],
   "source": [
    "d2v_model.train(X, total_examples=d2v_model.corpus_count, epochs=d2v_model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.75 * len(X))\n",
    "test_size = len(X) - train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_arrays = np.zeros((train_size, 50))\n",
    "test_text_arrays = np.zeros((test_size, 50))\n",
    "\n",
    "train_labels = np.zeros(train_size)\n",
    "test_labels = np.zeros(test_size)\n",
    "\n",
    "for i in range(train_size):\n",
    "    train_text_arrays[i] = d2v_model.docvecs['Train_' + str(i)]\n",
    "    train_labels[i] = preprocessed_data.label[i]\n",
    "    \n",
    "for i in range(test_size):\n",
    "    test_text_arrays[i] = d2v_model.docvecs['Train_' + str(train_size + i)]\n",
    "    test_labels[i] = preprocessed_data.label[train_size + i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_clf = BernoulliNB()\n",
    "nb_clf.fit(train_text_arrays, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = nb_clf.predict(test_text_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4828347906120571"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_clf.score(train_text_arrays, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "717"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.where(c == 0)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2717"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_clf = GaussianNB()\n",
    "nb_clf.fit(train_text_arrays, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5491946617579383"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_clf.score(train_text_arrays, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5458225984541774"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_clf.score(test_text_arrays, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(preprocessed_data)\n",
    "\n",
    "train_size = int(0.75 * len(preprocessed_data))\n",
    "cv_size = int(0.15 * len(preprocessed_data))\n",
    "test_size = len(preprocessed_data) - train_size - cv_size\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english', min_df=10, binary=True)\n",
    "dtm_matrix = vectorizer.fit_transform(preprocessed_data.title.values)\n",
    "\n",
    "train_data = dtm_matrix[:train_size]\n",
    "train_labels = preprocessed_data.label.values[:train_size]\n",
    "\n",
    "cv_data = dtm_matrix[train_size:train_size + cv_size]\n",
    "cv_labels = preprocessed_data.label.values[train_size:train_size + cv_size]\n",
    "\n",
    "test_data = dtm_matrix[train_size + cv_size:]\n",
    "test_labels = preprocessed_data.label.values[train_size + cv_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10186x1753 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 64801 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headline after vectorization: \n",
      "  (0, 1040)\t1\n",
      "  (0, 1747)\t1\n",
      "  (0, 1584)\t1\n",
      "  (0, 270)\t1\n",
      "  (0, 918)\t1\n",
      "  (0, 235)\t1\n",
      "  (0, 1313)\t1\n",
      "  (0, 713)\t1\n"
     ]
    }
   ],
   "source": [
    "print('Headline after vectorization: \\n{}'.format(train_data[124]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_clf = BernoulliNB()\n",
    "nb_clf.fit(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-6.88993069, -7.98854298, -5.68595789, ..., -7.07225225,\n",
       "        -4.87502767, -6.04263283],\n",
       "       [-6.41952949, -5.72638231, -6.75600172, ..., -6.75600172,\n",
       "        -6.75600172, -6.41952949]])"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_clf.feature_log_prob_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 2.71828183],\n",
       "       [2.71827987, 1.00000072],\n",
       "       [2.71828177, 1.00000002],\n",
       "       [1.        , 2.71828183],\n",
       "       [1.        , 2.71828183],\n",
       "       [2.71828067, 1.00000043],\n",
       "       [2.71827388, 1.00000292],\n",
       "       [2.71828143, 1.00000015],\n",
       "       [2.71828128, 1.0000002 ],\n",
       "       [1.        , 2.71828183]])"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(nb_clf.predict_proba(cv_data))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 1, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9987237384645592"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_clf.score(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_pred = nb_clf.predict(cv_data.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(cv_pred == cv_labels)/len(cv_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title      donald trump win year month day old first full...\n",
       "author                                          James Staten\n",
       "content    archives michael television donald trump win y...\n",
       "label                                                      1\n",
       "Name: 10189, dtype: object"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_data.iloc[train_size+3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_pred[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = nb_clf.predict(test_data.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(test_pred == test_labels)/len(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title                 circus hague kiev europe bow migration\n",
       "author                  Jafe Arnoldski (noreply@blogger.com)\n",
       "content    november eduard popov fort russ translated arn...\n",
       "label                                                      1\n",
       "Name: 12226, dtype: object"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_data.iloc[train_size+cv_size+3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(preprocessed_data))\n",
    "test_size = len(preprocessed_data) - train_size\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english', min_df=100, binary=True)\n",
    "dtm_matrix = vectorizer.fit_transform(preprocessed_data.content.values)\n",
    "\n",
    "train_data = dtm_matrix[:train_size]\n",
    "train_labels = preprocessed_data.label.values[:train_size]\n",
    "\n",
    "test_data = dtm_matrix[train_size:]\n",
    "test_labels = preprocessed_data.label.values[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10865x5556 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 2529123 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_clf = BernoulliNB()\n",
    "nb_clf.fit(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8237459733087897"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_clf.score(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = nb_clf.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8211262421788738"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(test_pred == test_labels)/len(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"feature probabilities for fake news\"\"\"\n",
    "highest_posterior_prob_indices = np.flip(np.argsort(np.exp(nb_clf.feature_log_prob_)[1]))[:20]\n",
    "lowest_posterior_prob_indices = np.argsort(np.exp(nb_clf.feature_log_prob_)[1])[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5556"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_clf.feature_count_.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['come', 'day', 'election', 'know', 'like', 'make', 'new',\n",
       "        'november', 'people', 'president', 'right', 'say', 'state', 'time',\n",
       "        'trump', 'use', 'way', 'work', 'world', 'year'], dtype='<U15')]"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.zeros(nb_clf.feature_count_.shape[1])\n",
    "\n",
    "for i in highest_posterior_prob_indices:\n",
    "    arr[i] = 1\n",
    "vectorizer.inverse_transform(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['broadway', 'bronx', 'dancer', 'exhibition', 'janeiro', 'leather',\n",
       "        'lineup', 'nytimes', 'onetime', 'onstage', 'opera', 'playoff',\n",
       "        'rhythm', 'spicer', 'teammate', 'tex', 'tillerson', 'tournament',\n",
       "        'weekday', 'weekdays'], dtype='<U15')]"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.zeros(nb_clf.feature_count_.shape[1])\n",
    "\n",
    "for i in lowest_posterior_prob_indices:\n",
    "    arr[i] = 1\n",
    "vectorizer.inverse_transform(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4375, 3005, 5079, 3609, 5538, 2910, 3319,  947, 2798, 4741, 5303,\n",
       "       5513, 1279, 5188, 3366, 5507, 3824, 4261, 5427, 1645])"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.flip(np.argsort(np.exp(nb_clf.feature_log_prob_)[1]))[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
